---
title: "Final_Project: Customer Lifetime Value"
author: "Team Binalytics: Maggie Lu (yaol4) and Wenyan Zhao (wenyanz)"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#load library
library(dplyr)
library(knitr)
library(plyr)
library(ggplot2)
library(leaps)
library(glmnet)
library(caret)
library(pROC)
library(tidyverse)
library(klaR)
library(rpart)
library(randomForest)
library(lubridate)
#Load data
ltv = read.csv(file = 'ltv.csv', header = TRUE)
#suppress warnings
```

### Task 1: Attrition Model
> To build an attrition model, first we want to clean up the data a bit to aggregate the information of each customer within a specifically defined timeframe. Here we would like to define the time interval to be 1 month-- partially because it is easier to aggregate, and therefore we define the near future to be the next month, in accordance with our dataframe. We then split the data into a test set and a training set, and then determines which of the variables we want to look into using subset selection criterias-- AIC/BIC. Finally, we look into different classfication models-- logistic regression, Naive Bayes and random forest to see which one has the best accuracy and specificity

#### Pre-processing the Data
```{r message=FALSE, warning=FALSE}

#Create a new data frame capturing the end state and the time the customer's in the system
#First of all transform the data: 

df <- ltv %>%
  group_by(id) %>%
dplyr::  mutate(duration = ifelse(as.Date(date)==min(as.Date(date)),1,ceiling(((as.Date(date)-min(as.Date(date))))/30)))

df2 = ddply(df,c("id","gender","duration"),summarize, cancelled = max(status), days.count = length(date), page.visited=sum(pages),time.spent=sum(onsite),entered.time = sum(entered), cmplt.time = sum(completed),holiday = max(holiday))

df.new = ddply(df2,.(id),transform,csum.days= cumsum(days.count),csum.page=cumsum(page.visited),csum.onsite= cumsum(time.spent), csum.entered = cumsum(entered.time), csum.cmplt.time = cumsum(cmplt.time))

df.new$cancelled = mapvalues(df.new$cancelled, from=c('0','1','2'),to=c(0,0,1))

ltv.cleaned = df.new[,c(2,3,4,10,11,12,13,14,15)]
```

#### Processing and Methods
```{r message=FALSE, warning=FALSE}
#First split the dataset into a training set and a test set
data(ltv.cleaned)
## 70% of the sample size
smp_size <- floor(0.7 * nrow(ltv.cleaned))
## set the seed to make your partition reproductible
set.seed(123)
train_ind <- sample(seq_len(nrow(ltv.cleaned)), size = smp_size)
ltv.train <- ltv.cleaned[train_ind, ]
ltv.test <- ltv.cleaned[-train_ind, ]

#Best subset
df.subset <- regsubsets(cancelled ~ .,
               data = ltv.train,
               nbest = 1,    # 1 best model for each number of predictors
               nvmax = NULL,    # NULL for no limit on number of variables
               method = "exhaustive", really.big = TRUE)
df.subset.sum = summary(df.subset)
df.subset.sum

#AIC
which.min(df.subset.sum$cp)
names(coef(df.subset,which.min(df.subset.sum$cp)))
 
#BIC
which.min(df.subset.sum$bic)
names(coef(df.subset,which.min(df.subset.sum$bic)))
```
> This makes sense. We will then use several models to determine the fit.

#### Validation and Metrics
```{r message=FALSE, warning=FALSE}
#Method 1: Logistic Regressions
glm.fit = glm(cancelled ~ .,family = binomial(), data = ltv.train)
#fit the prediction to the test data, and transform the test data with a cut off of 0.5 to see the confusion matrix for fitting
test.pred.prec = predict(glm.fit,ltv.test,type='response')
test.pred = ifelse(test.pred.prec < 0.4,0,1)
confusionMatrix(data = test.pred, reference = ltv.test$cancelled)
#ROC plot
roc.obj = roc(ltv.test$cancelled, test.pred.prec)
plot.roc(roc.obj, legacy.axes=TRUE)
```
> 

```{r message=FALSE, warning=FALSE}
#Method 2: Naive Bayes
#suppress warnings
options(warn=-1)
ltv.train$cancelled <- factor(ltv.train$cancelled)
ltv.train.nb <- NaiveBayes(cancelled ~., data = ltv.train, usekernel = TRUE)
confusionMatrix(predict(ltv.train.nb, ltv.test, type="class")$class, ltv.test$cancelled)
```

```{r message=FALSE, warning=FALSE}
#Method 3: Random Forest
ltv.rf = randomForest(cancelled ~ .,data=ltv.train, importance=TRUE)
print(ltv.rf)

pred = predict(ltv.rf, ltv.test)
confusionMatrix(pred, ltv.test$cancelled)
```

### Task 2: Lifetime Value of A customer
```{r}
#another way to aggregate the data
df2.test  = ddply(ltv,c("id"),mutate,yr_status = ifelse(as.Date(date)<=min(as.Date(date))+365,"<1yr",">1yr"))
df2.test = ddply(df2.test,c("id","gender","yr_status"),summarize, cancelled = max(status), days.count = max(as.Date(date))-min(as.Date(date)),days.left = as.Date("2014-12-31")- max(as.Date(date)),page.visited=sum(pages),time.spent=sum(onsite),entered.time = sum(entered), cmplt.time = sum(completed),holiday = max(holiday))

```


```{r}
#first, find about customer behavior for a year for every customer
#then the y value would be the one that extend beyond the one year mark until the churn time. 

#this is user-level behavior that we are trying to predict, therefore group the data by user_id
df.2 = ddply(ltv,c("id","gender"),summarize, cancelled = max(status), days.count = max(as.Date(date))-min(as.Date(date)),days.left = as.Date("2014-12-31")- max(as.Date(date)),page.visited=sum(pages),time.spent=sum(onsite),entered.time = sum(entered), cmplt.time = sum(completed),holiday = max(holiday))
#filter out the customers who have bene using this service for at least a year-- these are the customers we want to measure value for
df2.filtered = df.2[df.2$days.count>365,]
#df2.filtered = ltv %>%
#      filter(id %in% df2.filtered$id)

df2.filtered$cancelled = mapvalues(df2.filtered$cancelled, from=c('0','1','2'),to=c(0,0,1))
#df.new$month = ifelse(round(df.new$csum.days/30)==0,1,round(df.new$csum.days/30))
df2.filtered$total.days.count =with(df2.filtered, ifelse(cancelled==0,days.count+days.left,days.count))

df2.filtered$page.py = df2.filtered$page.visited/as.numeric(df2.filtered$days.count)*365
df2.filtered$time.py = df2.filtered$time.spent/as.numeric(df2.filtered$days.count)*365
df2.filtered$entered.py = df2.filtered$entered.time/as.numeric(df2.filtered$days.count)*365
df2.filtered$cmplt.py = df2.filtered$cmplt.time/as.numeric(df2.filtered$days.count)*365
df2.filtered$hd.py = df2.filtered$holiday/as.numeric(df2.filtered$days.count)*365
df2.filtered$ltv.obs = (as.numeric(df2.filtered$days.count)-365)/as.numeric(df2.filtered$days.count)

df2.final = df2.filtered[,c(2,10,11,12,13,14,15,16)]
```

```{r}
ltv.var.names = c("gender","page.py", "time.py", "entered.py", "cmplt.py","hd.py")
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = pmax(1, cex.cor * r))
}

# Use panel.cor to display correlations in lower panel.
pairs(df2.final[,ltv.var.names], lower.panel = panel.cor)
```

### Task 3: Customer Segmentation Scheme
#### Processing and Methods
```{r}


```
#### Validation and Metrics
```{r}


```

### Summary of Findings
